{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578f58fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello word\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "from sklearn.model_selection import train_test_split \n",
    "import sys \n",
    "import os \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.manifold import TSNE \n",
    "from wordcloud import WorldCloud \n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Check if GPU is available \n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec22cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this for unzip and read the file\n",
    "try:\n",
    "    df=pd.read_csv(\"/content/test/IMDB Dataset.csv\")\n",
    "except:\n",
    "    # !wget https://github.com/SalvatoreRa/tutorial/blob/main/datasets/IMDB.zip?raw=true\n",
    "    # !unzip IMDB.zip?raw=true\n",
    "    pass\n",
    "\n",
    "df['sentiment_encoded'] = np.where(df['sentiment']=='positive',0,1)\n",
    "X,y = df['review'].values, df['sentiment_encoded'].values\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y, test_size=.2)\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,stratify=y_train, test_size=.1)\n",
    "y_train, y_val, y_test = np.array(y_train), np.array(y_val), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b14750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordclouds(df):\n",
    "    '''\n",
    "    Generate two word clouds from the 50 most frequent words in the list of positive and negative reviews respectively.\n",
    "\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Separating reviews by sentiment\n",
    "    positive_reviews = df[df['sentiment'] == 'positive']['review']\n",
    "    negative_reviews = df[df['sentiment'] == 'negative']['review']\n",
    "\n",
    "\n",
    "    def get_words(reviews):\n",
    "        all_words = []\n",
    "        for review in reviews:\n",
    "            review = re.sub(r\"[^\\w\\s]\", '', review)\n",
    "            review = re.sub(r\"\\d\", '', review)\n",
    "            words = review.split()\n",
    "            filtered_words = [word for word in words if word not in stop_words and len(word) > 1]\n",
    "            all_words.extend(filtered_words)\n",
    "        return all_words\n",
    "\n",
    "\n",
    "    positive_words = get_words(positive_reviews)\n",
    "    negative_words = get_words(negative_reviews)\n",
    "\n",
    "\n",
    "    positive_counts = Counter(positive_words)\n",
    "    negative_counts = Counter(negative_words)\n",
    "\n",
    "\n",
    "    positive_wordcloud = WordCloud(\n",
    "        width=400,\n",
    "        height=400,\n",
    "        max_words=200,\n",
    "        max_font_size=100,\n",
    "        background_color='white',\n",
    "        color_func=lambda *args, **kwargs: \"green\"\n",
    "    ).generate_from_frequencies(positive_counts)\n",
    "\n",
    "    negative_wordcloud = WordCloud(\n",
    "        width=400,\n",
    "        height=400,\n",
    "        max_words=200,\n",
    "        max_font_size=100,\n",
    "        background_color='white',\n",
    "        color_func=lambda *args, **kwargs: \"red\"\n",
    "    ).generate_from_frequencies(negative_counts)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "    plt.title('Positive Reviews')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "    plt.title('Negative Reviews')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('word_clouds.jpg', format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "generate_wordclouds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c025c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_review_length_by_sentiment(df):\n",
    "    '''\n",
    "    Plots histograms of the number of words per review for positive and negative reviews with summary statistics.\n",
    "\n",
    "    '''\n",
    "\n",
    "    positive_reviews = df[df['sentiment'] == 'positive']['review']\n",
    "    negative_reviews = df[df['sentiment'] == 'negative']['review']\n",
    "\n",
    "\n",
    "    def get_review_lengths(reviews):\n",
    "        return [len(review.split()) for review in reviews]\n",
    "\n",
    "\n",
    "    positive_lengths = get_review_lengths(positive_reviews)\n",
    "    negative_lengths = get_review_lengths(negative_reviews)\n",
    "\n",
    "\n",
    "    def get_summary_stats(lengths):\n",
    "        return {\n",
    "            'min': np.min(lengths),\n",
    "            'avg': np.mean(lengths),\n",
    "            'median': np.median(lengths),\n",
    "            'max': np.max(lengths)\n",
    "        }\n",
    "\n",
    "    pos_stats = get_summary_stats(positive_lengths)\n",
    "    neg_stats = get_summary_stats(negative_lengths)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot for positive reviews\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(positive_lengths, bins=30, color='green', edgecolor='black', alpha=0.7)\n",
    "    plt.title('Word Distribution for Positive Reviews')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.grid(True)\n",
    "    stats_text = f\"Min: {pos_stats['min']}\\nAvg: {pos_stats['avg']:.2f}\\nMedian: {pos_stats['median']}\\nMax: {pos_stats['max']}\"\n",
    "    plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10, verticalalignment='top', horizontalalignment='right', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    # Plot for negative reviews\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(negative_lengths, bins=30, color='red', edgecolor='black', alpha=0.7)\n",
    "    plt.title('Word Distribution for Negative Reviews')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.grid(True)\n",
    "    stats_text = f\"Min: {neg_stats['min']}\\nAvg: {neg_stats['avg']:.2f}\\nMedian: {neg_stats['median']}\\nMax: {neg_stats['max']}\"\n",
    "    plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10, verticalalignment='top', horizontalalignment='right', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('review_length.jpg', format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_review_length_by_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf088ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    '''\n",
    "    Cleaning of the review: remove non-alphanumeric characters, collapse whitespace, and remove digits.\n",
    "    '''\n",
    "    review = re.sub(r\"[^\\w\\s]\", ' ', review)  # Replace non-word characters with space\n",
    "    review = re.sub(r\"\\s+\", ' ', review)      # Replace multiple spaces with a single space\n",
    "    review = re.sub(r\"\\d\", '', review)        # Remove digits\n",
    "    return review.strip().lower()\n",
    "\n",
    "def tokenize_reviews(x_train, x_val, x_test):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # tokenize and clean list of reviews\n",
    "    def tokenize_and_filter(reviews):\n",
    "        word_list = []\n",
    "        for review in reviews:\n",
    "            words = word_tokenize(preprocess_review(review))\n",
    "            filtered_words = [word for word in words if word not in stop_words and len(word) > 1]\n",
    "            word_list.extend(filtered_words)\n",
    "        return word_list\n",
    "\n",
    "    # Create a corpus\n",
    "    corpus = Counter(tokenize_and_filter(x_train))\n",
    "    # Select the 1000 most common words\n",
    "    vocab = {word: i+1 for i, word in enumerate([word for word, freq in corpus.most_common(1000)])}\n",
    "\n",
    "    # convert reviews into sequences of indices\n",
    "    def vectorize_reviews(reviews):\n",
    "        vectorized = []\n",
    "        for review in reviews:\n",
    "            tokenized = word_tokenize(preprocess_review(review))\n",
    "            indexed = [vocab[word] for word in tokenized if word in vocab]\n",
    "            vectorized.append(indexed)\n",
    "        return vectorized\n",
    "\n",
    "    _x_train = vectorize_reviews(x_train)\n",
    "    _x_val = vectorize_reviews(x_val)\n",
    "    _x_test = vectorize_reviews(x_test)\n",
    "\n",
    "    return _x_train, _x_val, _x_test, vocab\n",
    "\n",
    "\n",
    "x_train, x_val, x_test, vocab = tokenize_reviews(x_train, x_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e624ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_review_length_distribution(tokenized_reviews):\n",
    "    '''\n",
    "    Plots a histogram of the lengths of tokenized reviews and includes a box with summary statistics.\n",
    "\n",
    "    '''\n",
    "\n",
    "    review_lengths = [len(review) for review in tokenized_reviews]\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    min_length = np.min(review_lengths)\n",
    "    avg_length = np.mean(review_lengths)\n",
    "    median_length = np.median(review_lengths)\n",
    "    max_length = np.max(review_lengths)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(review_lengths, bins=30, color='blue', edgecolor='black', alpha=0.7)\n",
    "    plt.title('Distribution of Review Lengths')\n",
    "    plt.xlabel('Number of Tokens')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "    stats_text = f'Min Length: {min_length}\\nAverage Length: {avg_length:.2f}\\nMedian Length: {median_length}\\nMax Length: {max_length}'\n",
    "    plt.gca().text(0.95, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10, verticalalignment='top', horizontalalignment='right', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='wheat', alpha=0.5))\n",
    "    plt.savefig('review_length_after_tokenization.jpg', format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_review_length_distribution(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bcc500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(reviews, max_seq):\n",
    "    features = np.zeros((len(reviews), max_seq),dtype=int)\n",
    "    for ii, review in enumerate(reviews):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:max_seq]\n",
    "    return np.array(features)\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(padding_(x_train,500)), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(padding_(x_val,500)), torch.from_numpy(y_val))\n",
    "test_data = TensorDataset(torch.from_numpy(padding_(x_test,500)), torch.from_numpy(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=50)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=50)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3decec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim, drop_prob=0.5):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=self.hidden_dim,\n",
    "                          num_layers=no_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        embeds = self.embedding(x)\n",
    "        rnn_out, hidden = self.rnn(embeds, hidden)\n",
    "        rnn_out = rnn_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(rnn_out)\n",
    "        sig_out = self.sig(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        return sig_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "\n",
    "        h0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        return h0\n",
    "\n",
    "# Hyperparameters\n",
    "no_layers = 3\n",
    "vocab_size = len(vocab) + 1\n",
    "embedding_dim = 300\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "# Initialize the model\n",
    "model = SentimentRNN(no_layers, vocab_size, hidden_dim, embedding_dim, drop_prob=0.5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tsne = padding_(x_train,500)\n",
    "x_train_tsne = x_train_tsne[:1000,:]\n",
    "y_train_tsne= y_train[:1000]\n",
    "\n",
    "def plot_embeddings(x_train, y_train, model, device, batch_size=50):\n",
    "    model.eval()\n",
    "    embeddings_list = []\n",
    "\n",
    "    # Create a DataLoader to handle the x_train data in batches\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_train),\n",
    "                               torch.from_numpy(y_train))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for x_batch, _ in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            hidden = model.init_hidden(x_batch.size(0))\n",
    "\n",
    "            # Feed forward through the model to get to the embeddings layer\n",
    "            embeds = model.embedding(x_batch)\n",
    "            rnn_out, hidden = model.rnn(embeds, hidden)\n",
    "            rnn_out = rnn_out.contiguous().view(-1, model.hidden_dim)  # Flatten the output\n",
    "            out = model.dropout(rnn_out)\n",
    "            linear_output = model.fc(out)\n",
    "\n",
    "            embeddings_list.append(linear_output.cpu())  # Store CPU data\n",
    "\n",
    "    # Concatenate all batch embeddings into a single matrix\n",
    "    all_embeddings = torch.cat(embeddings_list, dim=0)\n",
    "\n",
    "    all_embeddings = all_embeddings.view(-1, 500)\n",
    "\n",
    "    # Reduce dimensions to 2D using t-SNE for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(all_embeddings.numpy())\n",
    "\n",
    "    df = pd.DataFrame(data=embeddings_2d, columns=['TSNE-1', 'TSNE-2'])\n",
    "    df['label'] = y_train\n",
    "    custom_palette = {0: 'green', 1: 'red'}\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = sns.scatterplot(data=df, x='TSNE-1', y='TSNE-2', hue='label', palette=custom_palette , s=60, alpha=0.6)\n",
    "    plt.title('2D t-SNE Visualization of Sentence Embeddings')\n",
    "    plt.xlabel('t-SNE dimension 1')\n",
    "    plt.ylabel('t-SNE dimension 2')\n",
    "    plt.legend(title='Label', bbox_to_anchor=(1.05, 1), loc=2)\n",
    "    plt.savefig('tsne_model_untrained_projection.jpg', format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_embeddings(x_train_tsne, y_train_tsne, model, device, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cdfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def acc(pred, label):\n",
    "    \"\"\"Calculate accuracy by comparing predicted labels with true labels.\"\"\"\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "clip = 5\n",
    "epochs = 5\n",
    "valid_loss_min = np.inf\n",
    "\n",
    "\n",
    "epoch_tr_loss, epoch_vl_loss = [], []\n",
    "epoch_tr_acc, epoch_vl_acc = [], []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    # Initialize hidden state\n",
    "    h = model.init_hidden(50)\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Detach hidden states\n",
    "        h = h.data\n",
    "\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = acc(output, labels)\n",
    "        train_acc += accuracy\n",
    "\n",
    "        # Clip gradients to prevent exploding gradient issues in RNNs\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_h = model.init_hidden(50)\n",
    "\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Detach hidden states\n",
    "        val_h = val_h.data\n",
    "\n",
    "        output, val_h = model(inputs, val_h)\n",
    "        val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        accuracy = acc(output, labels)\n",
    "        val_acc += accuracy\n",
    "\n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc / len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc / len(valid_loader.dataset)\n",
    "\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'Train Loss: {epoch_train_loss} Val Loss: {epoch_val_loss}')\n",
    "    print(f'Train Accuracy: {epoch_train_acc * 100}% Val Accuracy: {epoch_val_acc * 100}%')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b74252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, data_loader, device):\n",
    "    \"\"\"Predict output for a batch of data using the RNN model.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            hidden = model.init_hidden(batch_size).to(device)\n",
    "\n",
    "            output, _ = model(inputs, hidden)\n",
    "\n",
    "            predicted_probs = torch.sigmoid(output)\n",
    "            predicted_labels = (predicted_probs > 0.60).float()\n",
    "\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return predictions, true_labels, predicted_probs, labels\n",
    "\n",
    "\n",
    "predictions, true_labels, predicted_probs, labels = predict_batch(model, test_loader, device)\n",
    "print(f'Accuracy on test set: {accuracy_score(true_labels, predictions)}')\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.jpg', format='jpeg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b69829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# === Review preprocessing (fresh for inference) ===\n",
    "def preprocess_review(review):\n",
    "    \"\"\"\n",
    "    Clean a single review:\n",
    "    - remove non-alphanumeric chars\n",
    "    - collapse whitespace\n",
    "    - remove digits\n",
    "    - lowercase\n",
    "    \"\"\"\n",
    "    review = re.sub(r\"[^\\w\\s]\", ' ', review)  # Replace non-word characters with space\n",
    "    review = re.sub(r\"\\s+\", ' ', review)      # Replace multiple spaces with a single space\n",
    "    review = re.sub(r\"\\d\", '', review)        # Remove digits\n",
    "    return review.strip().lower()\n",
    "\n",
    "def vectorize_review(review, vocab, max_seq=500):\n",
    "    \"\"\"\n",
    "    Convert a single review into indices based on vocab.\n",
    "    \"\"\"\n",
    "    review = preprocess_review(review)\n",
    "    tokens = word_tokenize(review)\n",
    "    indexed = [vocab[word] for word in tokens if word in vocab]\n",
    "\n",
    "    # Pad/truncate to max_seq (same logic as training)\n",
    "    features = torch.zeros(max_seq, dtype=torch.long)\n",
    "    if len(indexed) > 0:\n",
    "        features[-len(indexed):] = torch.tensor(indexed[:max_seq])\n",
    "    return features.unsqueeze(0)  # shape (1, seq_len)\n",
    "\n",
    "# === Inference ===\n",
    "def predict_sentiment(model, review, vocab, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = vectorize_review(review, vocab).to(device)\n",
    "        hidden = model.init_hidden(1).to(device)\n",
    "        output, _ = model(input_tensor, hidden)\n",
    "\n",
    "        prob = torch.sigmoid(output).item()\n",
    "        sentiment = \"Positive\" if prob >= threshold else \"Negative\"\n",
    "    return sentiment, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9600f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [\n",
    "    \"The movie was fantastic, I really enjoyed it!\",\n",
    "    \"This was a waste of time, totally boring.\",\n",
    "    \"I hate it\",\n",
    "    \"I love it\"\n",
    "]\n",
    "\n",
    "for txt in sample_reviews:\n",
    "    sentiment, confidence = predict_sentiment(model, txt, vocab, device)\n",
    "    print(f\"Review: {txt}\")\n",
    "    print(f\"Prediction: {sentiment} (confidence: {confidence:.4f})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9565c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [\n",
    "    \"The movie was fantastic, I really enjoyed it!\",\n",
    "    \"This was a waste of time, totally boring.\",\n",
    "    \"I hate it\",\n",
    "    \"I love it\"\n",
    "]\n",
    "\n",
    "for txt in sample_reviews:\n",
    "    sentiment, confidence = predict_sentiment(model, txt, vocab, device)\n",
    "    print(f\"Review: {txt}\")\n",
    "    print(f\"Prediction: {sentiment} (confidence: {confidence:.4f})\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
