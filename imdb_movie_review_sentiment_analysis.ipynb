{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14254989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# check if we have GPU available\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce69a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip and read the datafile\n",
    "try:\n",
    "    df=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "except:\n",
    "    # !wget https://github.com/SalvatoreRa/tutorial/blob/main/datasets/IMDB.zip?raw=true\n",
    "    # !unzip IMDB.zip?raw=true\n",
    "    pass\n",
    "\n",
    "df['sentiment_encoded'] = np.where(df['sentiment']=='positive', 0,1)\n",
    "X,y = df['review'].values, df['sentimend_encoded'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.2)\n",
    "x_train,x_val,y_train, y_val = train_test_split(x_train, y_train, stratify=y_train, test_size=0.1)\n",
    "y_train, y_val, y_test = np.array(y_train), np.array(y_val), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordclouds(df):\n",
    "    '''\n",
    "    Generate two wqord clouds from the 50 most frequent words in the list of positive and negative reviews respectively\n",
    "    '''\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Separating reciews by sentiment\n",
    "    positive_reviews = df[df['sentiment'] == 'positive']['review']\n",
    "    negative_reviews = df[df['sentiment'] == 'negative']['review']\n",
    "\n",
    "    def get_words(reviews):\n",
    "        all_words = []\n",
    "        for review in reviews:\n",
    "            review = re.sub(r\"[^\\w\\s]\", '', review)\n",
    "            review = re.sub(r\"\\d\", '', review)\n",
    "            words = review.split()\n",
    "            filtered_words = [word for word in words if word not in stop_words and len(word) > 1]\n",
    "            all_words.extend(filtered_words)\n",
    "        return all_words\n",
    "    \n",
    "    positive_words = get_words(positive_reviews)\n",
    "    negative_words = get_words(negative_reviews)\n",
    "\n",
    "    positive_counts = Counter(positive_words)\n",
    "    negative_counts = Counter(negative_words)\n",
    "\n",
    "    positive_wordcloud = WordCloud(\n",
    "        width=400,\n",
    "        height=400,\n",
    "        max_words=200,\n",
    "        max_font_size=100,\n",
    "        background_color='white',\n",
    "        color_func=lambda *args, **kwargs: \"green\"\n",
    "    ).generate_from_frequencies(positive_counts)\n",
    "\n",
    "    negative_wordcloud = WordCloud(\n",
    "        width=400,\n",
    "        height=400,\n",
    "        max_words=200,\n",
    "        max_font_size=100,\n",
    "        background_color='white',\n",
    "        color_func=lambda *args, **kwargs: \"red\"\n",
    "    ).generate_from_frequencies(negative_counts)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "    plt.title(\"Positive Reviews\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "    plt.title('Negative Reviews')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('word_clouds.jpg', format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "generate_wordclouds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a269d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting review length by sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    ''' \n",
    "    Cleaning of reviews: remove non-alphanumeric characters, collapse whitespace, and remove digits\n",
    "    '''\n",
    "\n",
    "    review = re.sub(r\"[^\\w\\s]\", ' ', review) # Replcace non-word characters with space\n",
    "    review = re.sub(r\"\\s+\", ' ', review) # Replace multiple spaces with a single space\n",
    "    review = re.sub(r\"\\d\", '', review) # Remove digits\n",
    "    return review.strip().lower()\n",
    "\n",
    "def tokenize_reviews(x_train, x_val, x_test):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # tokenize and clean list of reviews\n",
    "    def tokenize_and_filter(reviews):\n",
    "        word_list = []\n",
    "        for review in reviews:\n",
    "            words = word_tokenize(preprocess_review(review))\n",
    "            filtered_words = [word for word in words if words not in stop_words and len(word) > 1]\n",
    "            word_list.extend(filtered_words)\n",
    "        return word_list\n",
    "    \n",
    "    # create a corpus\n",
    "    corpus = Counter(tokenize_and_filter(x_train))\n",
    "    # select the 1000 most commond words\n",
    "    vocab = {word: i+1 for i, word in enumerate ([word for word, freq in corpus.most_common(1000)])}\n",
    "\n",
    "    # convert reviews into sequences of indices\n",
    "    def vectorize_reviews(reviews):\n",
    "        vectorized = []\n",
    "        for review in reviews:\n",
    "            tokenized = word_tokenize(preprocess_review(review))\n",
    "            indexed = [vocab[word] for word in tokenized if word in vocab]\n",
    "        return vectorized\n",
    "    \n",
    "    _x_train = vectorize_reviews(x_train)\n",
    "    _x_val = vectorize_reviews(x_val)\n",
    "    _x_test = vectorize_reviews(x_test)\n",
    "\n",
    "    return _x_train, _x_val, _x_test, vocab\n",
    "\n",
    "X_train, x_val, x_test, vocab = tokenize_reviews(x_train, x_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a review length of the tokenized data distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
