{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14254989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# check if we have GPU available\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce69a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip and read the datafile\n",
    "try:\n",
    "    df=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "except:\n",
    "    # !wget https://github.com/SalvatoreRa/tutorial/blob/main/datasets/IMDB.zip?raw=true\n",
    "    # !unzip IMDB.zip?raw=true\n",
    "    pass\n",
    "\n",
    "df['sentiment_encoded'] = np.where(df['sentiment']=='positive', 0,1)\n",
    "X,y = df['review'].values, df['sentimend_encoded'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.2)\n",
    "x_train,x_val,y_train, y_val = train_test_split(x_train, y_train, stratify=y_train, test_size=0.1)\n",
    "y_train, y_val, y_test = np.array(y_train), np.array(y_val), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordclouds(df):\n",
    "    '''\n",
    "    Generate two wqord clouds from the 50 most frequent words in the list of positive and negative reviews respectively\n",
    "    '''\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Separating reciews by sentiment\n",
    "    positive_reviews = df[df['sentiment'] == 'positive']['review']\n",
    "    negative_reviews = df[df['sentiment'] == 'negative']['review']\n",
    "\n",
    "    def get_words(reviews):\n",
    "        all_words = []\n",
    "        for review in reviews:\n",
    "            review = re.sub(r\"[^\\w\\s]\", '', review)\n",
    "            review = re.sub(r\"\\d\", '', review)\n",
    "            words = review.split()\n",
    "            filtered_words = [word for word in words if word not in stop_words and len(word) > 1]\n",
    "            all_words.extend(filtered_words)\n",
    "        return all_words\n",
    "    \n",
    "    positive_words = get_words(positive_reviews)\n",
    "    negative_words = get_words(negative_reviews)\n",
    "\n",
    "    positive_counts = Counter(positive_words)\n",
    "    negative_counts = Counter(negative_words)\n",
    "\n",
    "    positive_wordcloud = WordCloud(\n",
    "        width=400,\n",
    "        height=400,\n",
    "        max_words=200,\n",
    "        max_font_size=100,\n",
    "        background_color='white',\n",
    "        color_func=lambda *args, **kwargs: \"green\"\n",
    "    ).generate_from_frequencies(positive_counts)\n",
    "\n",
    "    negative_wordcloud = WordCloud(\n",
    "        width=400,\n",
    "        height=400,\n",
    "        max_words=200,\n",
    "        max_font_size=100,\n",
    "        background_color='white',\n",
    "        color_func=lambda *args, **kwargs: \"red\"\n",
    "    ).generate_from_frequencies(negative_counts)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "    plt.title(\"Positive Reviews\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "    plt.title('Negative Reviews')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('word_clouds.jpg', format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "generate_wordclouds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a269d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting review length by sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    ''' \n",
    "    Cleaning of reviews: remove non-alphanumeric characters, collapse whitespace, and remove digits\n",
    "    '''\n",
    "\n",
    "    review = re.sub(r\"[^\\w\\s]\", ' ', review) # Replcace non-word characters with space\n",
    "    review = re.sub(r\"\\s+\", ' ', review) # Replace multiple spaces with a single space\n",
    "    review = re.sub(r\"\\d\", '', review) # Remove digits\n",
    "    return review.strip().lower()\n",
    "\n",
    "def tokenize_reviews(x_train, x_val, x_test):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # tokenize and clean list of reviews\n",
    "    def tokenize_and_filter(reviews):\n",
    "        word_list = []\n",
    "        for review in reviews:\n",
    "            words = word_tokenize(preprocess_review(review))\n",
    "            filtered_words = [word for word in words if words not in stop_words and len(word) > 1]\n",
    "            word_list.extend(filtered_words)\n",
    "        return word_list\n",
    "    \n",
    "    # create a corpus\n",
    "    corpus = Counter(tokenize_and_filter(x_train))\n",
    "    # select the 1000 most commond words\n",
    "    vocab = {word: i+1 for i, word in enumerate ([word for word, freq in corpus.most_common(1000)])}\n",
    "\n",
    "    # convert reviews into sequences of indices\n",
    "    def vectorize_reviews(reviews):\n",
    "        vectorized = []\n",
    "        for review in reviews:\n",
    "            tokenized = word_tokenize(preprocess_review(review))\n",
    "            indexed = [vocab[word] for word in tokenized if word in vocab]\n",
    "        return vectorized\n",
    "    \n",
    "    _x_train = vectorize_reviews(x_train)\n",
    "    _x_val = vectorize_reviews(x_val)\n",
    "    _x_test = vectorize_reviews(x_test)\n",
    "\n",
    "    return _x_train, _x_val, _x_test, vocab\n",
    "\n",
    "X_train, x_val, x_test, vocab = tokenize_reviews(x_train, x_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a review length of the tokenized data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim, drop_prob=0.5):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=self.hidden_dim,\n",
    "                          num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig == nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        embeds = self.embedding(x)\n",
    "        rnn_out, hidden = self.rnn(embeds, hidden)\n",
    "        out = self.dropout(rnn_out)\n",
    "        sig_out = self.sig(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''Initializes hidden state'''\n",
    "        h0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "no_layers = 3\n",
    "vocab_size = len(vocab) + 1\n",
    "embedding_dim = 300\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "#Initialize the model\n",
    "model = SentimentRNN(no_layers, vocab_size, hidden_dim, embedding_dim, drop_prob=0.5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff886da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting sentence dimensions on tSNE\n",
    "x_train_tsne = padding_(x_train, 500)\n",
    "y_train_tsne = x_train_tsne[:1000, :]\n",
    "y_train_tsne = y_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dedf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of the model\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def acc(pred, label):\n",
    "    \"\"\"Calculate accuracy by comparing predicted labels with true labels.\"\"\"\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "clip = 5\n",
    "epochs = 5\n",
    "valid_loss_min = np.inf \n",
    "\n",
    "epoch_tr_loss, epoch_vl_loss = [], []\n",
    "epoch_tr_acc,  epoch_vl_acc = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train() # Set the model to training mode\n",
    "\n",
    "    # Initialize hidden state\n",
    "    h = model.init_hidden(50)\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # detach hidden states\n",
    "        h = h.data \n",
    "\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backwards()\n",
    "        train_losses.append(loss.items())\n",
    "\n",
    "        # claculate accuracy\n",
    "        accuracy = acc(output, labels)\n",
    "        train_acc += accuracy\n",
    "\n",
    "        # clip gradients to prevent exploding gradient issues in RNNs\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()  # Set model to evaluation model\n",
    "    val_h = model.init_hidden(50)\n",
    "\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # detach hidden states \n",
    "        val_h = val_h.data\n",
    "\n",
    "        output, val_h = model(inputs, val_h)\n",
    "        val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        accuracy = acc(output, labels)\n",
    "        val_acc += accuracy\n",
    "    \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc / len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc / len(valid_loader.dataset)\n",
    "\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'Train Loss: {epoch_train_loss} Val Loss: {epoch_val_loss}')\n",
    "    print(f'Train Accuracy: {epoch_train_acc * 100}% Val Accuracy: {epoch_val_acc * 100}%')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, data_loader,  device):\n",
    "    \"\"\"Predict output for a batch of data using the RNN Model\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    tru_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            batch_size = inputs.size(8)\n",
    "\n",
    "            hidden = model.init_hidden(batch_size).to(device)\n",
    "\n",
    "            output, _ = model(inputs, hidden)\n",
    "\n",
    "            predicted_probs = torch.sigmoid(output)\n",
    "            predicted_labels = (predicted_probs > 0.60).float()\n",
    "\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return predictions, tru_labels, predicted_probs, labels\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
