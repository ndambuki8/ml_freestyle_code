{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578f58fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello word\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e401c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk gensim matplotlib seaborn adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f3f518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap-learn\n",
      "  Downloading umap_learn-0.5.9.post2-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 KB\u001b[0m \u001b[31m216.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.6 in ./env/lib/python3.10/site-packages (from umap-learn) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.23 in ./env/lib/python3.10/site-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.10/site-packages (from umap-learn) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.3.1 in ./env/lib/python3.10/site-packages (from umap-learn) (1.13.1)\n",
      "Collecting numba>=0.51.2\n",
      "  Downloading numba-0.62.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting llvmlite<0.46,>=0.45.0dev0\n",
      "  Downloading llvmlite-0.45.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m397.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in ./env/lib/python3.10/site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./env/lib/python3.10/site-packages (from scikit-learn>=1.6->umap-learn) (3.5.0)\n",
      "Installing collected packages: llvmlite, numba, pynndescent, umap-learn\n",
      "Successfully installed llvmlite-0.45.0 numba-0.62.0 pynndescent-0.5.13 umap-learn-0.5.9.post2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4009be",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhierarchy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dendrogram, linkage\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01madjustText\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adjust_text\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[1;32m     15\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm \n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from adjustText import adjust_text\n",
    "from umap import UMAP\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf38a4",
   "metadata": {},
   "source": [
    "### Getting IMDB dataset from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da25865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #this for unzip and read the file\n",
    "# !wget https://github.com/SalvatoreRa/tutorial/blob/main/datasets/IMDB.zip?raw=true\n",
    "# !unzip IMDB.zip?raw=true\n",
    "df=pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ff399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_reviews(reviews):\n",
    "\n",
    "    \"\"\"\n",
    "    simple preprocessing: splitting on the space and remove word less than 1 chr\n",
    "    \"\"\"\n",
    "\n",
    "    processed_reviews = []\n",
    "\n",
    "    for review in tqdm(reviews):\n",
    "        review = re.sub('<[^>]+>', '', review)\n",
    "        processed = re.sub('[^a-zA-Z ]', '', review)\n",
    "        words = processed.split()\n",
    "        processed_reviews.append(' '.join([word.lower() for word in words if len(word) > 1]))\n",
    "    return processed_reviews\n",
    "\n",
    "df['reviews_processed'] = preprocessing_reviews(df['review'])\n",
    "df['tokens'] = df['reviews_processed'].apply(nltk.word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Check if we GPU available\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this for unzip and read the file\n",
    "try:\n",
    "    df=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "except:\n",
    "    !wget https://github.com/SalvatoreRa/tutorial/blob/main/datasets/IMDB.zip?raw=true\n",
    "    !unzip IMDB.zip?raw=true\n",
    "\n",
    "df['sentiment_encoded'] = np.where(df['sentiment']=='positive',0,1)\n",
    "X,y = df['review'].values, df['sentiment_encoded'].values\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y, test_size=.2)\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,stratify=y_train, test_size=.1)\n",
    "y_train, y_val, y_test = np.array(y_train), np.array(y_val), np.array(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
